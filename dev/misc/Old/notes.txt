yc1 = full(sparse(y,1:100,1f0))

# Weight initialization for multiple layers: h=array of layer sizes
# Output is an array [w0,b0,w1,b1,...,wn,bn] where wi,bi is the weight matrix and bias vector for the i'th layer
function winit(h...)  # use winit(x,h1,h2,...,hn,y) for n hidden layer model
    w = Any[]
    for i=2:length(h)
        push!(w, xavier(h[i],h[i-1]))
        push!(w, zeros(h[i],1))
    end
    map(Atype, w)
end;

function convnet(w,x; pdrop=(0,0,0))    # pdrop[1]:input, pdrop[2]:conv, pdrop[3]:fc
    for i=1:2:length(w)
        if ndims(w[i]) == 4     # convolutional layer
            x = dropout(x, pdrop[i==1?1:2])
            x = conv4(w[i],x) .+ w[i+1]
            x = pool(relu.(x))
        elseif ndims(w[i]) == 2 # fully connected layer
            x = dropout(x, pdrop[i==1?1:3])
            x = w[i]*mat(x) .+ w[i+1]
            if i < length(w)-1; x = relu.(x); end
        else
            error("Unknown layer type: $(size(w[i]))")
        end
    end
    return x
end;

# Read the imdb dictionary and print the words
imdbvocab = Array{String}(length(imdbdict))
for (k,v) in imdbdict; imdbvocab[v]=k; end
map(a->imdbvocab[a], xtrn)

function onehotrows(idx, embeddings)
    nrows,ncols = length(idx), size(embeddings,1)
    z = zeros(Float32,nrows,ncols)
    @inbounds for i=1:nrows
        z[i,idx[i]] = 1
    end
    oftype(AutoGrad.getval(embeddings),z)
end

#"/home/kpmurphy/.julia/conda/3/bin"
# push!(LOAD_PATH, 


https://github.com/TuringLang/TuringTutorials/blob/master/3_BayesNN.ipynb

https://discourse.julialang.org/t/learning-bayesian-data-analysis-using-julia/5370/23

https://discourse.julialang.org/t/psa-replacement-of-ind2sub-sub2ind-in-julia-0-7/14666

RSA key with id efb66595564c5646f8fae19c4d619d60bc6b850d not found

soss.jl: https://cscherrer.github.io/
mxnet J1.0: https://github.com/apache/incubator-mxnet/issues/13836

https://julialang.org/learning/
http://ucidatascienceinitiative.github.io/IntroToJulia/

using NBInclude
@nbinclude("myfile.ipynb")
